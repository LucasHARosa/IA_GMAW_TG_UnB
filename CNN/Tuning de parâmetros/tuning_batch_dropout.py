# -*- coding: utf-8 -*-
"""Tuning_regressao4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FCVSoFzdg8cBw92DMmu9Y8BIo0oF9BRy

# Hiperparâmetros: Batch size e dropout


A camada de dropout é uma técnica regularizadora usada em redes neurais para evitar o overfitting. Ela ajuda a reduzir a dependência excessiva entre os neurônios, permitindo que diferentes subconjuntos de neurônios sejam ativados durante o treinamento. A importância do ajuste reside em encontrar um equilíbrio entre a regularização e a capacidade de aprendizado da rede. Um valor muito baixo de dropout pode não fornecer uma regularização adequada, enquanto um valor muito alto pode prejudicar a capacidade do modelo de aprender informações relevantes.

Portanto, é necessário ajustar o hiperparâmetro de acordo com o problema em questão, a complexidade do modelo e o tamanho do conjunto de dados. Em geral, é recomendado começar com um valor de dropout entre 0,2 e 0,5 e, em seguida, ajustá-lo com base no desempenho do modelo durante o treinamento e validação.

O tamanho do lote (batch size) refere-se à quantidade de exemplos de treinamento usados em cada passo de atualização dos pesos durante o treinamento da rede neural. É uma configuração importante que influencia o tempo de treinamento, a estabilidade do treinamento e a utilização eficiente dos recursos computacionais.

Tamanhos de lote maiores podem fornecer estimativas de gradiente mais estáveis, reduzindo a variância dos gradientes calculados em cada atualização de peso. No entanto, tamanhos de lote menores podem ajudar a evitar mínimos locais e a melhorar a generalização, permitindo atualizações de peso mais frequentes.
"""

# importações que deve er para criação da rede
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
#Importações para auxiliar
import numpy as np
import pandas as pd
# importações para tuning
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import GridSearchCV
from keras.layers.normalization.batch_normalization import BatchNormalization


#bibliotecas para baixar imagens
import os
from PIL import Image

trainDf = pd.read_excel("/content/drive/MyDrive/TCC/Experimento1.xlsx")
#print(trainDf)
#colTrain = ["LeftPoca","WidthPoca","Curto"]
y_train = trainDf[["percentageWidthPoca","Imagem"]].values

images = []
labels = []

image_dir = '/content/drive/MyDrive/TCC/Experimento1'


for image_file in os.listdir(image_dir):
    #print(image_file)
    image_path = os.path.join(image_dir, image_file)
    image = Image.open(image_path)
    image = image.resize((230, 230))  # Redimensionar para o tamanho desejado
    image = np.array(image)  # Converter a imagem em um array numpy

    # Adicionar a imagem e o rótulo correspondente às listas
    images.append(image)
    labels.append(image_file)

y_train = np.array(y_train)
y_trainOrdenado = sorted(y_train, key=lambda x: x[1])
y_trainOrdenado = np.array(y_trainOrdenado)
y_trainOrdenado2 = pd.DataFrame(y_trainOrdenado)
y_trainOrdenado3 = y_trainOrdenado2.drop(1,axis=1)
y_train = np.array(y_trainOrdenado3)
y_train = y_train *100
y_train = y_train.astype('float32')

X_train = np.array(images)
X_train = X_train.reshape(X_train.shape[0], 230, 230, 1)
X_train = X_train / 255.0
X_train32 = X_train.astype('float32')

def create_model(dropout):
    model = Sequential()

    model.add(Conv2D(16, (3, 3), activation='tanh',kernel_initializer='random_uniform', input_shape=(230, 230, 1)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(32, (3, 3), activation='tanh',kernel_initializer='random_uniform'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='tanh',kernel_initializer='random_uniform'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='tanh',kernel_initializer='random_uniform'))
    model.add(MaxPooling2D((2, 2)))

    model.add(Flatten())

    model.add(Dense(units = 500, activation='tanh', kernel_initializer='he_uniform'))
    model.add(Dropout(dropout))



    model.add(Dense(1, activation='linear'))

    model.compile(optimizer='Adamax', loss='mean_squared_error')
    return model

param_grid = {
    'batch_size':[10,20,32,64,128],
    'dropout':[0.1,0.15,0.2,0.3,0.4],
    'epochs': [4]
}

regressor = KerasRegressor(build_fn=create_model)
grid = GridSearchCV(estimator=regressor, param_grid=param_grid,scoring='neg_mean_squared_error',cv=5)

grid.fit(X_train32,y_train)
melhores_parametros = grid.best_params_
# mostra o valor do melhor resultado
melhor_precisao = grid.best_score_

resultados = grid.cv_results_
for indice_combinacao, score_mean, score_std, params in zip(resultados['rank_test_score'], resultados['mean_test_score'], resultados['std_test_score'], resultados['params']):
    print(f"Combinação {indice_combinacao}: Desempenho médio = {score_mean:.4f}, Desvio padrão = {score_std:.4f}, Hiperparâmetros = {params}")

print(melhor_precisao)
print(melhores_parametros)